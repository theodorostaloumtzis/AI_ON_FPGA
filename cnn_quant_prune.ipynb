{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b54b4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:53:24.786391: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-06 15:53:24.844799: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 15:53:24.846780: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-04-06 15:53:24.846786: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-04-06 15:53:25.160522: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-06 15:53:25.160562: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-06 15:53:25.160566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.regularizers import l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac9bcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, n_classes, filters_per_conv_layer, neurons_per_dense_layer):\n",
    "    x = x_in = Input(input_shape)\n",
    "    for i, f in enumerate(filters_per_conv_layer):\n",
    "        x = Conv2D(f, (3, 3), strides=(1, 1), padding='valid',\n",
    "                   kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001),\n",
    "                   use_bias=False, name=f'conv_{i}')(x)\n",
    "        x = BatchNormalization(name=f'bn_conv_{i}')(x)\n",
    "        x = Activation('relu', name=f'conv_act_{i}')(x)\n",
    "        x = MaxPooling2D(pool_size=(2, 2), name=f'pool_{i}')(x)\n",
    "    x = Flatten()(x)\n",
    "    for i, n in enumerate(neurons_per_dense_layer):\n",
    "        x = Dense(n, kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001),\n",
    "                  use_bias=False, name=f'dense_{i}')(x)\n",
    "        x = BatchNormalization(name=f'bn_dense_{i}')(x)\n",
    "        x = Activation('relu', name=f'dense_act_{i}')(x)\n",
    "    x = Dense(n_classes, name='output_dense')(x)\n",
    "    x_out = Activation('softmax', name='output_softmax')(x)\n",
    "    return Model(inputs=[x_in], outputs=[x_out], name='baseline_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc736868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, val_data, epochs=10):\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_data, epochs=epochs, validation_data=val_data)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b4708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, begin_step=0, end_step=2000, final_sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Prune the model using TensorFlow Model Optimization Toolkit.\n",
    "    Args:\n",
    "        model: The model to be pruned.\n",
    "        begin_step: The step at which pruning starts.\n",
    "        end_step: The step at which pruning ends.\n",
    "        final_sparsity: The final sparsity level.\n",
    "    Returns:\n",
    "        pruned_model: The pruned model.\n",
    "        callbacks: Callbacks for pruning.\n",
    "    \"\"\"\n",
    "    pruning_schedule = tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0,\n",
    "        final_sparsity=final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step,\n",
    "    )\n",
    "    pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, pruning_schedule=pruning_schedule)\n",
    "    pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    callbacks = [\n",
    "        tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    "    return pruned_model, callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1930a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to strip and quantize the pruned model\n",
    "def strip_and_qat_model(pruned_model, train_data, val_data, epochs=5):\n",
    "    \"\"\"Activation-aware quantization aware training (QAT) of the pruned model.\"\"\"\n",
    "    stripped_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "    quantize_model = tfmot.quantization.keras.quantize_model\n",
    "    qat_model = quantize_model(stripped_model)\n",
    "    qat_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    qat_model.fit(train_data, epochs=epochs, validation_data=val_data)\n",
    "    return qat_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338747bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tflite_model(model, filename='model_quant.tflite'):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(f\"Saved quantized model to: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411eb023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 15:53:26.020326: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2025-04-06 15:53:26.020349: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-04-06 15:53:26.020360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (theodoros-MS-7D75): /proc/driver/nvidia/version does not exist\n",
      "2025-04-06 15:53:26.020558: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test  = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape to add channel dimension (28x28x1)\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test  = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "# Split off a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create tf.data.Dataset objects (optional but recommended for performance)\n",
    "batch_size = 1024\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "val_data   = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "test_data  = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Optional: set number of epochs\n",
    "n_epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "646eba50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/theodoros/Documents/AI_ON_FPGA/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "Epoch 1/10\n",
      "53/53 [==============================] - 3s 37ms/step - loss: 1.2752 - accuracy: 0.6580 - val_loss: 1.7185 - val_accuracy: 0.7772\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.4832 - accuracy: 0.9021 - val_loss: 1.4233 - val_accuracy: 0.5745\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.3222 - accuracy: 0.9402 - val_loss: 1.2044 - val_accuracy: 0.6235\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.2600 - accuracy: 0.9549 - val_loss: 0.9122 - val_accuracy: 0.7442\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.2266 - accuracy: 0.9636 - val_loss: 0.5945 - val_accuracy: 0.8443\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.2034 - accuracy: 0.9690 - val_loss: 0.3916 - val_accuracy: 0.9082\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.1871 - accuracy: 0.9728 - val_loss: 0.2803 - val_accuracy: 0.9427\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.1750 - accuracy: 0.9760 - val_loss: 0.2196 - val_accuracy: 0.9607\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.1651 - accuracy: 0.9782 - val_loss: 0.2030 - val_accuracy: 0.9648\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.1568 - accuracy: 0.9799 - val_loss: 0.1885 - val_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# 1. Build baseline model\n",
    "model = build_model(input_shape=(28, 28, 1), n_classes=10,\n",
    "                    filters_per_conv_layer=[16, 16, 24],\n",
    "                    neurons_per_dense_layer=[42, 64])\n",
    "\n",
    "# 2. Apply QAT BEFORE training\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "qat_model = quantize_model(model)\n",
    "\n",
    "# 3. Compile and train as usual\n",
    "qat_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = qat_model.fit(train_data, validation_data=val_data, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ef0642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"baseline_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " quantize_layer (QuantizeLay  (None, 28, 28, 1)        3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_conv_0 (QuantizeWrapp  (None, 26, 26, 16)       177       \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_bn_conv_0 (QuantizeWr  (None, 26, 26, 16)       65        \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv_act_0 (QuantizeW  (None, 26, 26, 16)       3         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_pool_0 (QuantizeWrapp  (None, 13, 13, 16)       1         \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_conv_1 (QuantizeWrapp  (None, 11, 11, 16)       2337      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_bn_conv_1 (QuantizeWr  (None, 11, 11, 16)       65        \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv_act_1 (QuantizeW  (None, 11, 11, 16)       3         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_pool_1 (QuantizeWrapp  (None, 5, 5, 16)         1         \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_conv_2 (QuantizeWrapp  (None, 3, 3, 24)         3505      \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_bn_conv_2 (QuantizeWr  (None, 3, 3, 24)         97        \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv_act_2 (QuantizeW  (None, 3, 3, 24)         3         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_pool_2 (QuantizeWrapp  (None, 1, 1, 24)         1         \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWrap  (None, 24)               1         \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dense_0 (QuantizeWrap  (None, 42)               1011      \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_bn_dense_0 (QuantizeW  (None, 42)               171       \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_act_0 (Quantize  (None, 42)               3         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_dense_1 (QuantizeWrap  (None, 64)               2691      \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_bn_dense_1 (QuantizeW  (None, 64)               259       \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_act_1 (Quantize  (None, 64)               3         \n",
      " WrapperV2)                                                      \n",
      "                                                                 \n",
      " quant_output_dense (Quantiz  (None, 10)               655       \n",
      " eWrapperV2)                                                     \n",
      "                                                                 \n",
      " quant_output_softmax (Quant  (None, 10)               1         \n",
      " izeWrapperV2)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,056\n",
      "Trainable params: 10,574\n",
      "Non-trainable params: 482\n",
      "_________________________________________________________________\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1926 - accuracy: 0.9680\n",
      "QAT Model Test Accuracy: 0.9680\n"
     ]
    }
   ],
   "source": [
    "qat_model.summary()\n",
    "# 4. Evaluate the quantized model\n",
    "loss, acc = qat_model.evaluate(test_data)\n",
    "print(f\"QAT Model Test Accuracy: {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
