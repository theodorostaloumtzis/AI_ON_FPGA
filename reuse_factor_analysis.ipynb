{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b867f484",
   "metadata": {},
   "source": [
    "### Load the trainned prunned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541f2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from utils.nn_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd2550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'models'\n",
    "model_path = f'{model_dir}/quantized_pruned_cnn_model.h5'\n",
    "# model_path = f'{model_dir}/quantized_svhn.h5'\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "\n",
    "\n",
    "qmodel = tf.keras.models.load_model(model_path, custom_objects=co)\n",
    "qmodel = strip_pruning(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c54ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"qkeras\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " fused_convbn_0 (QConv2DBat  (None, 26, 26, 8)         113       \n",
      " chnorm)                                                         \n",
      "                                                                 \n",
      " conv_act_0 (QActivation)    (None, 26, 26, 8)         0         \n",
      "                                                                 \n",
      " pool_0 (MaxPooling2D)       (None, 13, 13, 8)         0         \n",
      "                                                                 \n",
      " fused_convbn_1 (QConv2DBat  (None, 11, 11, 16)        1233      \n",
      " chnorm)                                                         \n",
      "                                                                 \n",
      " conv_act_1 (QActivation)    (None, 11, 11, 16)        0         \n",
      "                                                                 \n",
      " pool_1 (MaxPooling2D)       (None, 5, 5, 16)          0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_0 (QDense)            (None, 48)                19200     \n",
      "                                                                 \n",
      " bn_dense_0 (BatchNormaliza  (None, 48)                192       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_act_0 (QActivation)   (None, 48)                0         \n",
      "                                                                 \n",
      " dense_1 (QDense)            (None, 24)                1152      \n",
      "                                                                 \n",
      " bn_dense_1 (BatchNormaliza  (None, 24)                96        \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " dense_act_1 (QActivation)   (None, 24)                0         \n",
      "                                                                 \n",
      " output_dense (Dense)        (None, 10)                250       \n",
      "                                                                 \n",
      " output_softmax (Activation  (None, 10)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22236 (86.87 KB)\n",
      "Trainable params: 22042 (86.10 KB)\n",
      "Non-trainable params: 194 (784.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81c50d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer: fused_convbn_0 (QConv2DBatchnorm) - Total MACs: 72\n",
      " ReuseFactor |  SerialPct\n",
      "---------------------------\n",
      "           1 |     0.0139\n",
      "           2 |     0.0278\n",
      "           3 |     0.0417\n",
      "           4 |     0.0556\n",
      "           6 |     0.0833\n",
      "           8 |     0.1111\n",
      "           9 |     0.1250\n",
      "          12 |     0.1667\n",
      "          18 |     0.2500\n",
      "          24 |     0.3333\n",
      "          36 |     0.5000\n",
      "          72 |     1.0000\n",
      "\n",
      "Layer: fused_convbn_1 (QConv2DBatchnorm) - Total MACs: 1152\n",
      " ReuseFactor |  SerialPct\n",
      "---------------------------\n",
      "           1 |     0.0009\n",
      "           2 |     0.0017\n",
      "           3 |     0.0026\n",
      "           4 |     0.0035\n",
      "           6 |     0.0052\n",
      "           8 |     0.0069\n",
      "           9 |     0.0078\n",
      "          12 |     0.0104\n",
      "          16 |     0.0139\n",
      "          18 |     0.0156\n",
      "          24 |     0.0208\n",
      "          32 |     0.0278\n",
      "          36 |     0.0312\n",
      "          48 |     0.0417\n",
      "          64 |     0.0556\n",
      "          72 |     0.0625\n",
      "          96 |     0.0833\n",
      "         128 |     0.1111\n",
      "         144 |     0.1250\n",
      "         192 |     0.1667\n",
      "         288 |     0.2500\n",
      "         384 |     0.3333\n",
      "         576 |     0.5000\n",
      "        1152 |     1.0000\n",
      "\n",
      "Layer: dense_0 (QDense) - Total MACs: 19200\n",
      " ReuseFactor |  SerialPct\n",
      "---------------------------\n",
      "           1 |     0.0001\n",
      "           2 |     0.0001\n",
      "           3 |     0.0002\n",
      "           4 |     0.0002\n",
      "           5 |     0.0003\n",
      "           6 |     0.0003\n",
      "           8 |     0.0004\n",
      "          10 |     0.0005\n",
      "          12 |     0.0006\n",
      "          15 |     0.0008\n",
      "          16 |     0.0008\n",
      "          20 |     0.0010\n",
      "          24 |     0.0013\n",
      "          25 |     0.0013\n",
      "          30 |     0.0016\n",
      "          32 |     0.0017\n",
      "          40 |     0.0021\n",
      "          48 |     0.0025\n",
      "          50 |     0.0026\n",
      "          60 |     0.0031\n",
      "          64 |     0.0033\n",
      "          75 |     0.0039\n",
      "          80 |     0.0042\n",
      "          96 |     0.0050\n",
      "         100 |     0.0052\n",
      "         120 |     0.0063\n",
      "         128 |     0.0067\n",
      "         150 |     0.0078\n",
      "         160 |     0.0083\n",
      "         192 |     0.0100\n",
      "         200 |     0.0104\n",
      "         240 |     0.0125\n",
      "         256 |     0.0133\n",
      "         300 |     0.0156\n",
      "         320 |     0.0167\n",
      "         384 |     0.0200\n",
      "         400 |     0.0208\n",
      "         480 |     0.0250\n",
      "         600 |     0.0312\n",
      "         640 |     0.0333\n",
      "         768 |     0.0400\n",
      "         800 |     0.0417\n",
      "         960 |     0.0500\n",
      "        1200 |     0.0625\n",
      "        1280 |     0.0667\n",
      "        1600 |     0.0833\n",
      "        1920 |     0.1000\n",
      "        2400 |     0.1250\n",
      "        3200 |     0.1667\n",
      "        3840 |     0.2000\n",
      "        4800 |     0.2500\n",
      "        6400 |     0.3333\n",
      "        9600 |     0.5000\n",
      "       19200 |     1.0000\n",
      "\n",
      "Layer: dense_1 (QDense) - Total MACs: 1152\n",
      " ReuseFactor |  SerialPct\n",
      "---------------------------\n",
      "           1 |     0.0009\n",
      "           2 |     0.0017\n",
      "           3 |     0.0026\n",
      "           4 |     0.0035\n",
      "           6 |     0.0052\n",
      "           8 |     0.0069\n",
      "           9 |     0.0078\n",
      "          12 |     0.0104\n",
      "          16 |     0.0139\n",
      "          18 |     0.0156\n",
      "          24 |     0.0208\n",
      "          32 |     0.0278\n",
      "          36 |     0.0312\n",
      "          48 |     0.0417\n",
      "          64 |     0.0556\n",
      "          72 |     0.0625\n",
      "          96 |     0.0833\n",
      "         128 |     0.1111\n",
      "         144 |     0.1250\n",
      "         192 |     0.1667\n",
      "         288 |     0.2500\n",
      "         384 |     0.3333\n",
      "         576 |     0.5000\n",
      "        1152 |     1.0000\n",
      "\n",
      "Layer: output_dense (Dense) - Total MACs: 240\n",
      " ReuseFactor |  SerialPct\n",
      "---------------------------\n",
      "           1 |     0.0042\n",
      "           2 |     0.0083\n",
      "           3 |     0.0125\n",
      "           4 |     0.0167\n",
      "           5 |     0.0208\n",
      "           6 |     0.0250\n",
      "           8 |     0.0333\n",
      "          10 |     0.0417\n",
      "          12 |     0.0500\n",
      "          15 |     0.0625\n",
      "          16 |     0.0667\n",
      "          20 |     0.0833\n",
      "          24 |     0.1000\n",
      "          30 |     0.1250\n",
      "          40 |     0.1667\n",
      "          48 |     0.2000\n",
      "          60 |     0.2500\n",
      "          80 |     0.3333\n",
      "         120 |     0.5000\n",
      "         240 |     1.0000\n"
     ]
    }
   ],
   "source": [
    "reuse_factors_with_serial_pct(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5062930",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_folder = 'Projects_Reuse_Factor_Analysis'\n",
    "project_folder = 'ProjectsRF'\n",
    "project_path = f'{project_folder}/RFanalysis0'\n",
    "backend = 'Vitis'\n",
    "default_precision = 'ap_fixed<16,6>'\n",
    "part = 'xck26-sfvc784-2LV-c'\n",
    "project_name = 'rf_analysis_0'\n",
    "#dataset='svhn'\n",
    "dataset='mnist'\n",
    "RF=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1895054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_4, layer type: InputLayer, input shapes: [[None, 28, 28, 1]], output shape: [None, 28, 28, 1]\n",
      "Layer name: fused_convbn_0, layer type: QConv2DBatchnorm, input shapes: [[None, 28, 28, 1]], output shape: [None, 26, 26, 8]\n",
      "Layer name: conv_act_0, layer type: Activation, input shapes: [[None, 26, 26, 8]], output shape: [None, 26, 26, 8]\n",
      "Layer name: pool_0, layer type: MaxPooling2D, input shapes: [[None, 26, 26, 8]], output shape: [None, 13, 13, 8]\n",
      "Layer name: fused_convbn_1, layer type: QConv2DBatchnorm, input shapes: [[None, 13, 13, 8]], output shape: [None, 11, 11, 16]\n",
      "Layer name: conv_act_1, layer type: Activation, input shapes: [[None, 11, 11, 16]], output shape: [None, 11, 11, 16]\n",
      "Layer name: pool_1, layer type: MaxPooling2D, input shapes: [[None, 11, 11, 16]], output shape: [None, 5, 5, 16]\n",
      "Layer name: flatten_3, layer type: Reshape, input shapes: [[None, 5, 5, 16]], output shape: [None, 400]\n",
      "Layer name: dense_0, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 48]\n",
      "Layer name: bn_dense_0, layer type: BatchNormalization, input shapes: [[None, 48]], output shape: [None, 48]\n",
      "Layer name: dense_act_0, layer type: Activation, input shapes: [[None, 48]], output shape: [None, 48]\n",
      "Layer name: dense_1, layer type: QDense, input shapes: [[None, 48]], output shape: [None, 24]\n",
      "Layer name: bn_dense_1, layer type: BatchNormalization, input shapes: [[None, 24]], output shape: [None, 24]\n",
      "Layer name: dense_act_1, layer type: Activation, input shapes: [[None, 24]], output shape: [None, 24]\n",
      "Layer name: output_dense, layer type: Dense, input shapes: [[None, 24]], output shape: [None, 10]\n",
      "Layer name: output_softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "fused_convbn_0        MACs=    72  ReuseFactor=1\n",
      "fused_convbn_1        MACs=  1152  ReuseFactor=1\n",
      "dense_0               MACs= 19200  ReuseFactor=1\n",
      "dense_1               MACs=  1152  ReuseFactor=1\n",
      "output_dense          MACs=   240  ReuseFactor=1\n",
      "Model\n",
      "  Precision:         ap_fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  input_4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "    Strategy:        Resource\n",
      "  fused_convbn_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<6,1,TRN,WRAP,0>\n",
      "      bias:          fixed<6,1,TRN,WRAP,0>\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    ParallelizationFactor:1\n",
      "    ConvImplementation:LineBuffer\n",
      "    Strategy:        Resource\n",
      "  fused_convbn_0_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  conv_act_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT,0>\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  pool_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    ConvImplementation:LineBuffer\n",
      "    Strategy:        Resource\n",
      "  fused_convbn_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<6,1,TRN,WRAP,0>\n",
      "      bias:          fixed<6,1,TRN,WRAP,0>\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    ParallelizationFactor:1\n",
      "    ConvImplementation:LineBuffer\n",
      "    Strategy:        Resource\n",
      "  fused_convbn_1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  conv_act_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT,0>\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  pool_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    ConvImplementation:LineBuffer\n",
      "    Strategy:        Resource\n",
      "  flatten_3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "    Strategy:        Resource\n",
      "  dense_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<6,1,TRN,WRAP,0>\n",
      "      bias:          auto\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Resource\n",
      "  dense_0_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  bn_dense_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      scale:         auto\n",
      "      bias:          auto\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Resource\n",
      "  dense_act_0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT,0>\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  dense_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<6,1,TRN,WRAP,0>\n",
      "      bias:          auto\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Resource\n",
      "  dense_1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  bn_dense_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      scale:         auto\n",
      "      bias:          auto\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Resource\n",
      "  dense_act_1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        ufixed<6,0,RND_CONV,SAT,0>\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  output_dense\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        auto\n",
      "      bias:          auto\n",
      "      accum:         auto\n",
      "    ReuseFactor:     1\n",
      "    Strategy:        Resource\n",
      "  output_dense_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Strategy:        Resource\n",
      "  output_softmax\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      table:         fixed<18,8,TRN,WRAP,0>\n",
      "      exp_table:     fixed<18,8,RND,SAT,0>\n",
      "      inv_table:     fixed<18,8,RND,SAT,0>\n",
      "    ReuseFactor:     1\n",
      "    TableSize:       1024\n",
      "    Implementation:  stable\n",
      "    Skip:            False\n",
      "    Strategy:        Resource\n",
      "Interpreting Model\n",
      "Topology:\n",
      "Layer name: input_4, layer type: InputLayer, input shapes: [[None, 28, 28, 1]], output shape: [None, 28, 28, 1]\n",
      "Layer name: fused_convbn_0, layer type: QConv2DBatchnorm, input shapes: [[None, 28, 28, 1]], output shape: [None, 26, 26, 8]\n",
      "Layer name: conv_act_0, layer type: Activation, input shapes: [[None, 26, 26, 8]], output shape: [None, 26, 26, 8]\n",
      "Layer name: pool_0, layer type: MaxPooling2D, input shapes: [[None, 26, 26, 8]], output shape: [None, 13, 13, 8]\n",
      "Layer name: fused_convbn_1, layer type: QConv2DBatchnorm, input shapes: [[None, 13, 13, 8]], output shape: [None, 11, 11, 16]\n",
      "Layer name: conv_act_1, layer type: Activation, input shapes: [[None, 11, 11, 16]], output shape: [None, 11, 11, 16]\n",
      "Layer name: pool_1, layer type: MaxPooling2D, input shapes: [[None, 11, 11, 16]], output shape: [None, 5, 5, 16]\n",
      "Layer name: flatten_3, layer type: Reshape, input shapes: [[None, 5, 5, 16]], output shape: [None, 400]\n",
      "Layer name: dense_0, layer type: QDense, input shapes: [[None, 400]], output shape: [None, 48]\n",
      "Layer name: bn_dense_0, layer type: BatchNormalization, input shapes: [[None, 48]], output shape: [None, 48]\n",
      "Layer name: dense_act_0, layer type: Activation, input shapes: [[None, 48]], output shape: [None, 48]\n",
      "Layer name: dense_1, layer type: QDense, input shapes: [[None, 48]], output shape: [None, 24]\n",
      "Layer name: bn_dense_1, layer type: BatchNormalization, input shapes: [[None, 24]], output shape: [None, 24]\n",
      "Layer name: dense_act_1, layer type: Activation, input shapes: [[None, 24]], output shape: [None, 24]\n",
      "Layer name: output_dense, layer type: Dense, input shapes: [[None, 24]], output shape: [None, 10]\n",
      "Layer name: output_softmax, layer type: Softmax, input shapes: [[None, 10]], output shape: [None, 10]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/theodoros/miniconda3/envs/ai_on_fpga/lib/python3.10/site-packages/keras/src/constraints.py:365: UserWarning: The `keras.constraints.serialize()` API should only be used for objects of type `keras.constraints.Constraint`. Found an instance of type <class 'qkeras.quantizers.quantized_bits'>, which may lead to improper serialization.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import hls4ml\n",
    "import utils.plotting as plotting\n",
    "\n",
    "# Generate base HLS config\n",
    "hls_config = hls4ml.utils.config_from_keras_model(\n",
    "    qmodel,\n",
    "    granularity='name',\n",
    "    backend=backend,\n",
    "    default_precision=default_precision\n",
    ")\n",
    "\n",
    "# Set model-level precision\n",
    "hls_config['Model']['Precision'] = default_precision\n",
    "\n",
    "# Force Resource strategy globally\n",
    "for lname, lcfg in hls_config['LayerName'].items():\n",
    "    lcfg['Strategy'] = 'Resource'\n",
    "\n",
    "# Inject computed reuse factors\n",
    "reuse_factors = reuse_percentage_to_factors(qmodel, serial_pct=RF)\n",
    "for lname, factor in reuse_factors.items():\n",
    "    if lname in hls_config['LayerName']:\n",
    "        hls_config['LayerName'][lname]['ReuseFactor'] = factor\n",
    "        \n",
    "    else:\n",
    "        print(f\"Warning: Layer {lname} not found in HLS config\")\n",
    "\n",
    "plotting.print_dict(hls_config)\n",
    "\n",
    "\n",
    "# Convert and compile\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    qmodel,\n",
    "    hls_config=hls_config,\n",
    "    backend=backend,\n",
    "    output_dir=project_path,\n",
    "    part=part,\n",
    "    io_type='io_stream',\n",
    "    clock_period=5,\n",
    "    trace=True,\n",
    "    project_name=project_name,\n",
    "    clock_uncertainty= \"12.5\"\n",
    ")\n",
    "hls_model.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09eabd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injected: ProjectsRF/RFanalysis0/firmware/rf_analysis_0_stream.cpp\n",
      "Injected: ProjectsRF/RFanalysis0/firmware/rf_analysis_0_stream.h\n",
      "Injected: ProjectsRF/RFanalysis0/rf_analysis_0_test.cpp\n",
      "Copied: gen_tb.py → ProjectsRF/RFanalysis0/gen_tb.py\n",
      "Copied: compute_performance.py → ProjectsRF/RFanalysis0/compute_performance.py\n",
      "Copied: build_prj.tcl → ProjectsRF/RFanalysis0/build_prj.tcl\n",
      "Copied: golden_preds.py → ProjectsRF/RFanalysis0/golden_preds.py\n"
     ]
    }
   ],
   "source": [
    "from template_injector import TemplateInjector\n",
    "\n",
    "injector = TemplateInjector(template_dir=\"templates\")\n",
    "\n",
    "injector.inject(\n",
    "    project_dir=project_path,\n",
    "    project_name=project_name,\n",
    "    force=True  # or False to skip existing files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "180f8a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/theodoros/Projects/AI_ON_FPGA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "print(f\"Current working directory: {pwd}\")\n",
    "build = True\n",
    "from utils.build import build_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51c8d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BUILD] Running: python gen_tb.py --dataset mnist --n-samples 100 --seed 42 --ap-total-bits 16 --ap-int-bits 6 --model keras_model.keras --save-labels\n",
      "        cwd=/home/theodoros/Projects/AI_ON_FPGA/ProjectsRF/RFanalysis0\n",
      "\n",
      "[BUILD] Running: vitis-run --mode hls --tcl build_prj.tcl\n",
      "        cwd=/home/theodoros/Projects/AI_ON_FPGA/ProjectsRF/RFanalysis0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mbuild_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/AI_ON_FPGA/utils/build.py:222\u001b[0m, in \u001b[0;36mbuild_project\u001b[0;34m(project_dir, dataset, n_samples, seed, model_path, ap_total_bits, ap_int_bits)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# 2) HLS: pick the right frontend\u001b[39;00m\n\u001b[1;32m    221\u001b[0m hls_cmd, hls_label \u001b[38;5;241m=\u001b[39m _pick_hls_cmd()\n\u001b[0;32m--> 222\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhls_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m02_hls_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhls_label\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✅ Build completed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/AI_ON_FPGA/utils/build.py:121\u001b[0m, in \u001b[0;36m_run\u001b[0;34m(cmd_list, cwd, log_prefix)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[BUILD] Running: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m        cwd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out, \u001b[38;5;28mopen\u001b[39m(err_f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 121\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# Prepare a short summary from the end of the logs\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtail\u001b[39m(p, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_on_fpga/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_on_fpga/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_on_fpga/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_on_fpga/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai_on_fpga/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if build:\n",
    "    build_project(project_dir=f\"{project_path}\",\n",
    "        dataset=dataset,\n",
    "        n_samples=100\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_on_fpga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
