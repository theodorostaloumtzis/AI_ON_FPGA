{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab669ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "os.environ['XILINX_VITIS'] = '/tools/Xilinx/Vitis/2024.2'\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vivado/2020.1/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = '/tools/Xilinx/Vitis_HLS/2024.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe13fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test  = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape to add channel dimension (28x28x1)\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test  = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test  = to_categorical(y_test, 10)\n",
    "\n",
    "# Split off a validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create tf.data.Dataset objects (optional but recommended for performance)\n",
    "batch_size = 1024\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "val_data   = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "test_data  = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "# Optional: set number of epochs\n",
    "n_epochs = 10\n",
    "\n",
    "train_size = len(x_train) \n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4eb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "filters_per_conv_layer = [16, 16, 24]\n",
    "neurons_per_dense_layer = [42, 64]\n",
    "\n",
    "x = x_in = Input(input_shape)\n",
    "\n",
    "for i, f in enumerate(filters_per_conv_layer):\n",
    "    print(('Adding convolutional block {} with N={} filters').format(i, f))\n",
    "    x = Conv2D(\n",
    "        int(f),\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        use_bias=False,\n",
    "        name='conv_{}'.format(i),\n",
    "    )(x)\n",
    "    x = BatchNormalization(name='bn_conv_{}'.format(i))(x)\n",
    "    x = Activation('relu', name='conv_act_%i' % i)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='pool_{}'.format(i))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "for i, n in enumerate(neurons_per_dense_layer):\n",
    "    print(('Adding dense block {} with N={} neurons').format(i, n))\n",
    "    x = Dense(n, kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001), name='dense_%i' % i, use_bias=False)(x)\n",
    "    x = BatchNormalization(name='bn_dense_{}'.format(i))(x)\n",
    "    x = Activation('relu', name='dense_act_%i' % i)(x)\n",
    "x = Dense(int(n_classes), name='output_dense')(x)\n",
    "x_out = Activation('softmax', name='output_softmax')(x)\n",
    "\n",
    "model = Model(inputs=[x_in], outputs=[x_out], name='keras_baseline')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if layer.__class__.__name__ in ['Conv2D', 'Dense']:\n",
    "        w = layer.get_weights()[0]\n",
    "        layersize = np.prod(w.shape)\n",
    "        print(\"{}: {}\".format(layer.name, layersize))  # 0 = weights, 1 = biases\n",
    "        if layersize > 4096:  # assuming that shape[0] is batch, i.e., 'None'\n",
    "            print(\"Layer {} is too large ({}), are you sure you want to train?\".format(layer.name, layersize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a8eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.sparsity import keras as sparsity\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "\n",
    "NSTEPS = int(train_size * 0.9) // batch_size  # 90% train, 10% validation in 10-fold cross validation\n",
    "print('Number of training steps per epoch is {}'.format(NSTEPS))\n",
    "\n",
    "\n",
    "# Prune all convolutional and dense layers gradually from 0 to 50% sparsity every 2 epochs,\n",
    "# ending by the 10th epoch\n",
    "def pruneFunction(layer):\n",
    "    pruning_params = {\n",
    "        'pruning_schedule': sparsity.PolynomialDecay(\n",
    "            initial_sparsity=0.0, final_sparsity=0.50, begin_step=NSTEPS * 2, end_step=NSTEPS * 10, frequency=NSTEPS\n",
    "        )\n",
    "    }\n",
    "    if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "    if isinstance(layer, tf.keras.layers.Dense) and layer.name != 'output_dense':\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer, **pruning_params)\n",
    "    return layer\n",
    "\n",
    "\n",
    "model_pruned = tf.keras.models.clone_model(model, clone_function=pruneFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dacc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acacd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True  # True if you want to retrain, false if you want to load a previsously trained model\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "save_path = os.path.join(models_path, 'pruned_cnn_model.h5')\n",
    "\n",
    "if train:\n",
    "    LOSS = tf.keras.losses.CategoricalCrossentropy()\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=3e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "\n",
    "    model_pruned.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "        pruning_callbacks.UpdatePruningStep(),\n",
    "    ]\n",
    "\n",
    "    start = time.time()\n",
    "    model_pruned.fit(train_data, epochs=n_epochs, validation_data=val_data, callbacks=callbacks)\n",
    "    end = time.time()\n",
    "\n",
    "    print('It took {} minutes to train Keras model'.format((end - start) / 60.0))\n",
    "    model_pruned.save(save_path)\n",
    "\n",
    "\n",
    "else:\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "    model_pruned = tf.keras.models.load_model('pruned_cnn_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79396070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import QActivation\n",
    "from qkeras import QDense, QConv2DBatchnorm\n",
    "\n",
    "x = x_in = Input(shape=input_shape)\n",
    "\n",
    "for i, f in enumerate(filters_per_conv_layer):\n",
    "    print(('Adding fused QConv+BN block {} with N={} filters').format(i, f))\n",
    "    x = QConv2DBatchnorm(\n",
    "        int(f),\n",
    "        kernel_size=(3, 3),\n",
    "        strides=(1, 1),\n",
    "        kernel_quantizer=\"quantized_bits(6,0,alpha=1)\",\n",
    "        bias_quantizer=\"quantized_bits(6,0,alpha=1)\",\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        use_bias=True,\n",
    "        name='fused_convbn_{}'.format(i),\n",
    "    )(x)\n",
    "    x = QActivation('quantized_relu(6)', name='conv_act_%i' % i)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2), name='pool_{}'.format(i))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "for i, n in enumerate(neurons_per_dense_layer):\n",
    "    print(('Adding QDense block {} with N={} neurons').format(i, n))\n",
    "    x = QDense(\n",
    "        n,\n",
    "        kernel_quantizer=\"quantized_bits(6,0,alpha=1)\",\n",
    "        kernel_initializer='lecun_uniform',\n",
    "        kernel_regularizer=l1(0.0001),\n",
    "        name='dense_%i' % i,\n",
    "        use_bias=False,\n",
    "    )(x)\n",
    "    x = BatchNormalization(name='bn_dense_{}'.format(i))(x)\n",
    "    x = QActivation('quantized_relu(6)', name='dense_act_%i' % i)(x)\n",
    "x = Dense(int(n_classes), name='output_dense')(x)\n",
    "x_out = Activation('softmax', name='output_softmax')(x)\n",
    "qmodel = Model(inputs=[x_in], outputs=[x_out], name='qkeras')\n",
    "\n",
    "qmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410f743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the quantized layers\n",
    "from qkeras.autoqkeras.utils import print_qmodel_summary\n",
    "\n",
    "print_qmodel_summary(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81f66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_pruned = tf.keras.models.clone_model(qmodel, clone_function=pruneFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec5451",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "q_save_path = os.path.join(models_path, 'quantized_pruned_cnn_model.h5')\n",
    "\n",
    "n_epochs = 30\n",
    "if train:\n",
    "    LOSS = tf.keras.losses.CategoricalCrossentropy()\n",
    "    OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=3e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=True)\n",
    "    qmodel_pruned.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "        pruning_callbacks.UpdatePruningStep(),\n",
    "    ]\n",
    "\n",
    "    start = time.time()\n",
    "    history = qmodel_pruned.fit(train_data, epochs=n_epochs, validation_data=val_data, callbacks=callbacks, verbose=1)\n",
    "    end = time.time()\n",
    "    print('\\n It took {} minutes to train!\\n'.format((end - start) / 60.0))\n",
    "\n",
    "    qmodel_pruned.save(q_save_path)\n",
    "\n",
    "else:\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "    qmodel_pruned = tf.keras.models.load_model('quantized_pruned_cnn_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5661d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_baseline = model_pruned.predict(x_test)\n",
    "test_score_baseline = model_pruned.evaluate(x_test, y_test)\n",
    "\n",
    "predict_qkeras = qmodel_pruned.predict(x_test)\n",
    "test_score_qkeras = qmodel_pruned.evaluate(x_test, y_test)\n",
    "\n",
    "print('Keras accuracy = {} , QKeras 6-bit accuracy = {}'.format(test_score_baseline[1], test_score_qkeras[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "labels = ['%i' % nr for nr in range(0, n_classes)]  # If you want to look at all the labels\n",
    "# labels = ['0','1','9'] # Look at only a few labels, here for digits 0, 1 and 9\n",
    "print('Plotting ROC for labels {}'.format(labels))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df_q = pd.DataFrame()\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "auc1 = {}\n",
    "fpr_q = {}\n",
    "tpr_q = {}\n",
    "auc1_q = {}\n",
    "%matplotlib inline\n",
    "colors = ['#67001f', '#b2182b', '#d6604d', '#f4a582', '#fddbc7', '#d1e5f0', '#92c5de', '#4393c3', '#2166ac', '#053061']\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "for i, label in enumerate(labels):\n",
    "    df[label] = y_test[:, int(label)]\n",
    "    df[label + '_pred'] = predict_baseline[:, int(label)]\n",
    "    fpr[label], tpr[label], threshold = metrics.roc_curve(df[label], df[label + '_pred'])\n",
    "    auc1[label] = metrics.auc(fpr[label], tpr[label])\n",
    "\n",
    "    df_q[label] = y_test[:, int(label)]\n",
    "    df_q[label + '_pred'] = predict_qkeras[:, int(label)]\n",
    "    fpr_q[label], tpr_q[label], threshold_q = metrics.roc_curve(df_q[label], df_q[label + '_pred'])\n",
    "    auc1_q[label] = metrics.auc(fpr_q[label], tpr_q[label])\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[label],\n",
    "        tpr[label],\n",
    "        label=r'{}, AUC Keras = {:.1f}% AUC QKeras = {:.1f}%)'.format(label, auc1[label] * 100, auc1_q[label] * 100),\n",
    "        linewidth=1.5,\n",
    "        c=colors[i],\n",
    "        linestyle='solid',\n",
    "    )\n",
    "    plt.plot(fpr_q[label], tpr_q[label], linewidth=1.5, c=colors[i], linestyle='dotted')\n",
    "\n",
    "plt.semilogx()\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.xlim(0.01, 1.0)\n",
    "plt.ylim(0.5, 1.1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.figtext(\n",
    "    0.2,\n",
    "    0.83,\n",
    "    r'Accuracy Keras = {:.1f}% QKeras 8-bit = {:.1f}%'.format(test_score_baseline[1] * 100, test_score_qkeras[1] * 100),\n",
    "    wrap=True,\n",
    "    horizontalalignment='left',\n",
    "    verticalalignment='center',\n",
    ")\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "lines = [Line2D([0], [0], ls='-'), Line2D([0], [0], ls='--')]\n",
    "from matplotlib.legend import Legend\n",
    "\n",
    "leg = Legend(ax, lines, labels=['Keras', 'QKeras'], loc='lower right', frameon=False)\n",
    "ax.add_artist(leg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20960c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWeights(model):\n",
    "    allWeightsByLayer = {}\n",
    "    for layer in model.layers:\n",
    "        if (layer._name).find(\"batch\") != -1 or len(layer.get_weights()) < 1:\n",
    "            continue\n",
    "        weights = layer.weights[0].numpy().flatten()\n",
    "        allWeightsByLayer[layer._name] = weights\n",
    "        print('Layer {}: % of zeros = {}'.format(layer._name, np.sum(weights == 0) / np.size(weights)))\n",
    "\n",
    "    labelsW = []\n",
    "    histosW = []\n",
    "\n",
    "    for key in reversed(sorted(allWeightsByLayer.keys())):\n",
    "        labelsW.append(key)\n",
    "        histosW.append(allWeightsByLayer[key])\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    bins = np.linspace(-1.5, 1.5, 50)\n",
    "    plt.hist(histosW, bins, histtype='stepfilled', stacked=True, label=labelsW, edgecolor='black')\n",
    "    plt.legend(frameon=False, loc='upper left')\n",
    "    plt.ylabel('Number of Weights')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.figtext(0.2, 0.38, model._name, wrap=True, horizontalalignment='left', verticalalignment='center')\n",
    "\n",
    "\n",
    "doWeights(model_pruned)\n",
    "doWeights(qmodel_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_wrapper\n",
    "\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "co['PruneLowMagnitude'] = pruning_wrapper.PruneLowMagnitude\n",
    "\n",
    "model = tf.keras.models.load_model(save_path, custom_objects=co)\n",
    "model = strip_pruning(model)\n",
    "\n",
    "qmodel = tf.keras.models.load_model(q_save_path, custom_objects=co)\n",
    "qmodel = strip_pruning(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = 'Projects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0dcbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "# Auto-generate base config\n",
    "hls_config = hls4ml.utils.config_from_keras_model(\n",
    "    model,\n",
    "    granularity='name',\n",
    "    backend='Vitis',\n",
    "    default_precision='ap_fixed<16,6>'\n",
    ")\n",
    "\n",
    "# Custom performance overrides\n",
    "for layer_name, layer_cfg in hls_config['LayerName'].items():\n",
    "    layer_cfg['Strategy'] = 'Latency'\n",
    "    layer_cfg['ReuseFactor'] = 32\n",
    "    if 'FifoDepth' not in layer_cfg:\n",
    "        layer_cfg['FifoDepth'] = 4\n",
    "\n",
    "plotting.print_dict(hls_config)\n",
    "\n",
    "save_proj_path = os.path.join(project_folder, 'Baseline')\n",
    "\n",
    "# Convert and compile\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model,\n",
    "    hls_config=hls_config,\n",
    "    backend='Vitis',\n",
    "    output_dir=save_proj_path,\n",
    "    part='xczu5ev-sfvc784-1-i',\n",
    "    io_type='io_stream',\n",
    "    clock_period=5,\n",
    "    trace=True,\n",
    "    output_format='vhdl'  # <- Force VHDL output\n",
    ")\n",
    "hls_model.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23603f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.utils.plot_model(hls_model, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e29bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hls4ml.model.profiling import numerical\n",
    "\n",
    "numerical(model=model, hls_model=hls_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate config from QKeras model\n",
    "hls_config_q = hls4ml.utils.config_from_keras_model(\n",
    "    qmodel,\n",
    "    granularity='name',\n",
    "    backend='Vitis',\n",
    ")\n",
    "\n",
    "# Inject optimizations\n",
    "hls_config_q['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "hls_config_q['Model']['PruneReuseFactorStrategy'] = 'load_balance'\n",
    "\n",
    "for lname, lcfg in hls_config_q['LayerName'].items():\n",
    "    lcfg['Strategy'] = 'Latency'\n",
    "    lcfg['ReuseFactor'] = 32\n",
    "    if 'FifoDepth' not in lcfg:\n",
    "        lcfg['FifoDepth'] = 4\n",
    "\n",
    "plotting.print_dict(hls_config_q)\n",
    "\n",
    "save_proj_path = os.path.join(project_folder, 'Quantized')\n",
    "\n",
    "# Convert and compile\n",
    "hls_model_q = hls4ml.converters.convert_from_keras_model(\n",
    "    qmodel,\n",
    "    hls_config=hls_config_q,\n",
    "    output_dir=save_proj_path,\n",
    "    backend='Vitis',\n",
    "    io_type='io_stream',\n",
    "    clock_period=5,\n",
    "    trace=True,\n",
    "    part = 'xczu5ev-sfvc784-1-i',\n",
    "    output_format='vhdl'  # <- Force VHDL output\n",
    ")\n",
    "\n",
    "hls_model_q.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical(model=qmodel, hls_model=hls_model_q)\n",
    "hls4ml.utils.plot_model(hls_model_q, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test)\n",
    "y_predict_hls4ml = hls_model.predict(np.ascontiguousarray(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_q = qmodel.predict(x_test)\n",
    "y_predict_hls4ml_q = hls_model_q.predict(np.ascontiguousarray(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f1ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def plotROC(Y, y_pred, y_pred_hls4ml, label=\"Model\"):\n",
    "    accuracy_keras = float(accuracy_score(np.argmax(Y, axis=1), np.argmax(y_pred, axis=1)))\n",
    "    accuracy_hls4ml = float(accuracy_score(np.argmax(Y, axis=1), np.argmax(y_pred_hls4ml, axis=1)))\n",
    "\n",
    "    print(\"Accuracy Keras:  {}\".format(accuracy_keras))\n",
    "    print(\"Accuracy hls4ml: {}\".format(accuracy_hls4ml))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "    _ = plotting.makeRoc(Y, y_pred, labels=['%i' % nr for nr in range(n_classes)])\n",
    "    plt.gca().set_prop_cycle(None)  # reset the colors\n",
    "    _ = plotting.makeRoc(Y, y_pred_hls4ml, labels=['%i' % nr for nr in range(n_classes)], linestyle='--')\n",
    "\n",
    "    from matplotlib.lines import Line2D\n",
    "\n",
    "    lines = [Line2D([0], [0], ls='-'), Line2D([0], [0], ls='--')]\n",
    "    from matplotlib.legend import Legend\n",
    "\n",
    "    leg = Legend(ax, lines, labels=['Keras', 'hls4ml'], loc='lower right', frameon=False)\n",
    "    ax.add_artist(leg)\n",
    "    plt.figtext(0.2, 0.38, label, wrap=True, horizontalalignment='left', verticalalignment='center')\n",
    "    plt.ylim(0.01, 1.0)\n",
    "    plt.xlim(0.7, 1.0)\n",
    "\n",
    "\n",
    "# Plot the pruned floating point model:\n",
    "plotROC(y_test, y_predict, y_predict_hls4ml, label=\"Keras\")\n",
    "\n",
    "# Plot the pruned and quantized QKeras model\n",
    "plotROC(y_test, y_predict_q, y_predict_hls4ml_q, label=\"QKeras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = True  # Only if you want to synthesize the models yourself (>1h per model) rather than look at the provided reports.\n",
    "if synth:\n",
    "    hls_model.build(csim=False, synth=True, vsynth=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth = True  # Only if you want to synthesize the models yourself (>1h per model) rather than look at the provided reports.\n",
    "if synth:\n",
    "    hls_model_q.build(csim=False, synth=True, vsynth=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
